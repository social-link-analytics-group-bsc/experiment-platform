{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adopted-consequence",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-toner",
   "metadata": {},
   "source": [
    "This is a notebook to see if there is an impact of the socio-demographic variables on the tendency for the subject to get `Right` or `Wrong`. \n",
    "It also serves as a recoding and cleaning notebook to obtain a clean dataset that can be reused for other analysis in the future. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-freedom",
   "metadata": {},
   "source": [
    "## Import modules and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-business",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import itertools \n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic analysis imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat analysis import \n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import scipy \n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.stattools as stools\n",
    "\n",
    "import statsmodels.stats as stats \n",
    "\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "from statsmodels.stats import multicomp as mc\n",
    "#from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "import pingouin as pg\n",
    "\n",
    "#from pymer4.models import Lmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for the notebook\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-protocol",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-swiss",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the folders \n",
    "data_folder = \"../data/\"\n",
    "analysis_results_folder = \"../results/\"\n",
    "plots_folders = \"../plots\"\n",
    "\n",
    "# Filenames\n",
    "dataset_filename = 'sondea_analisis_20210323_SONDEA.csv'\n",
    "dataset_clean_filename = 'cleaned_dataset_per_subject.csv'\n",
    "\n",
    "dataset_topic_filename = 'news _information _methodology_news.csv'\n",
    "dataset_news_filename = 'details_news.csv'\n",
    "dataset_news_full_filename = 'cleaned_dataset_per_news.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "df_raw = pd.read_csv('{}{}'.format(data_folder, dataset_filename),  sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-happiness",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load topic\n",
    "df_topic = pd.read_csv('{}{}'.format(data_folder, dataset_topic_filename), sep=';')\n",
    "df_topic = df_topic[['Titulo', 'General Topics ']]\n",
    "df_topic.columns = ['news_title', 'topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime columns to type datetime\n",
    "df_raw['dt_arrive'] = pd.to_datetime(df_raw['date_arrive'])\n",
    "df_raw['dt_finish'] = pd.to_datetime(df_raw['date_finish'])\n",
    "\n",
    "# create a new variable to store time completion (in minutes)\n",
    "df_raw['time_completion_min'] = df_raw.apply(lambda row: round((row['dt_finish'] - row['dt_arrive']).seconds/60,3), axis=1)\n",
    "\n",
    "#df_raw['dm_lugar'] = df_raw.apply(lambda row: row['dm_provincia'] if row['dm_provincia'] != 'Fuera de España' else unicodedata.normalize('NFD', row['dm_prov_otro'].strip().title()).encode('ascii', 'ignore').decode(), axis=1)\n",
    "df_raw['dm_lugar_country'] = df_raw.apply(lambda row: row['dm_prov_otro'] if row['dm_provincia'] == 'Fuera de España' else 'España', axis=1)\n",
    "#TODO [CCAA]\n",
    "\n",
    "df_raw['dm_employment'] = df_raw.apply(lambda row: row['dm_empleo'] if row['dm_empleo'] != 'Otro' else row['dm_empleo_otro'].strip().title(), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-clark",
   "metadata": {},
   "source": [
    "## Recoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-wednesday",
   "metadata": {},
   "source": [
    "### Recode True - Fake news "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-daniel",
   "metadata": {},
   "source": [
    "We recoded the different option of Fake news and True news and the answers to follow a easier format. Rather than having the answer being dependent to the question to know if it was a correct or no we did as following:\n",
    "\n",
    "1. Recode the type of news as either `True` new or `Fake` news. \n",
    "\n",
    "1. In case of the subject thought a `True` news as `True`, the answer was recoded as `Right`\n",
    "\n",
    "1. In case of the subject thought a `Fake` news was `Fake`, the answer was recoded as `Right`\n",
    "\n",
    "1. In case of the subject thought a `True` news was `Fake`, the answer was recoded as `Wrong`\n",
    "\n",
    "1. In case of the subject thought a `Fake` news was `True`, the answer was recoded as `Wrong`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the True - False and Fake news\n",
    "df_raw = df_raw.rename(columns={'tysno_verdadera': 'True news', 'fysno_verdadera': \"Fake news\"})\n",
    "\n",
    "# Recode the Si and No into Right and Wrong to avoid confusion\n",
    "df_raw['True news'] = df_raw['True news'].replace({'sí': 'Right'})\n",
    "df_raw['True news'] = df_raw['True news'].replace({'no': 'Wrong'})\n",
    "\n",
    "df_raw['Fake news'] = df_raw['Fake news'].replace({'sí': 'Wrong'})\n",
    "df_raw['Fake news'] = df_raw['Fake news'].replace({'no': 'Right'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-tuesday",
   "metadata": {},
   "source": [
    "### Create count_error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-participation",
   "metadata": {},
   "source": [
    "We created a variable called `count_error`. \n",
    "We simply check the number of time a subject did a mistake. \n",
    "* When the subject got `Right` to all answer, we coded `No error`\n",
    "* If a subject did a mistake (regardless of considering a Fake news as True, or a True news as Fake), we recoded as `1 error`\n",
    "* If a subject did 2 mistakes, we recoded as `2 errors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of error per profile\n",
    "df_raw['count_error_int'] = (df_raw[['True news', 'Fake news']]==\"Wrong\").sum(axis=\"columns\")\n",
    "df_raw['count_error'] = df_raw['count_error_int']\n",
    "df_raw['count_error'].value_counts()\n",
    "\n",
    "df_raw = df_raw.astype({'count_error': 'category'})\n",
    "# Transform the count errors into category type\n",
    "df_raw.count_error.replace({0: 'No error', 1:'1 error', 2:'2 errors'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_raw.count_error = pd.Categorical(df_raw.count_error, categories=['No error', '1 error', '2 errors'],ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-replication",
   "metadata": {},
   "source": [
    "### Create the 4 categories variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-assumption",
   "metadata": {},
   "source": [
    "We created another way to group subjects. \n",
    "Here we followed the idea to divide in the 4 possible scenario\n",
    "\n",
    "1. The subject got the `True` news `Right` and the `Fake` news `Right`: `TR-FR`\n",
    "\n",
    "1. The subject got the `True` news `Right` and the `Fake` news `Wrong`: `TR-FW`\n",
    "\n",
    "1. The subject got the `True` news `Wrong` and the `Fake` news `Right`: `TW-FR`\n",
    "\n",
    "1. The subject got the `True` news `Wrong` and the `Fake` news `Wrong`: `TW-FW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df_raw['True news'] == 'Right') & (df_raw['Fake news'] == 'Right'),\n",
    "    (df_raw['True news'] == 'Right') & (df_raw['Fake news'] == 'Wrong'),\n",
    "    (df_raw['True news'] == 'Wrong') & (df_raw['Fake news'] == 'Right'),\n",
    "    (df_raw['True news'] == 'Wrong') & (df_raw['Fake news'] == 'Wrong'),\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['TR-FR', 'TR-FW', 'TW-FR', 'TW-FW']\n",
    "df_raw['cat_tf_rw'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['cat_tf_rw'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-telephone",
   "metadata": {},
   "source": [
    "### Create the 2 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-chemistry",
   "metadata": {},
   "source": [
    "Here, created a category with only the `Right` and `Wrong` distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['r_w'] = np.where(\n",
    "    (df_raw['count_error_int'] > 0), 'W', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['r_w'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941dd24-6d61-4861-852e-b046646e9e49",
   "metadata": {},
   "source": [
    "### Recoge gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e6c71-9846-48c1-8936-5bf4697dd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.replace({\"Femenino\": \"Female\", 'Masculino': \"Male\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986aba9-5651-4941-ba9f-9918689eed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"Gender\"] = df_raw['dm_genero']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-senator",
   "metadata": {},
   "source": [
    "### Recode religion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-buddy",
   "metadata": {},
   "source": [
    "We decided to recode the religion into a dichotomic variable. The logic behind is that there were a lot of of `Católico` (n=512), `Ateo` (n=237) and `Agnóstico` (n=164), while other religions where low in number (n<10) (see the table under). \n",
    "We could do it different and keep the following coding as alternative (not done here) but it is a sensitive decision.\n",
    "\n",
    "* `Católico`\n",
    "\n",
    "* `Ateo`\n",
    "\n",
    "* `Agnóstico`\n",
    "\n",
    "* `Other faiths` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode religion into believers, atheist, agnostic\n",
    "\n",
    "## Get the value of Otro in dm_pref_religion\n",
    "df_raw['dm_pref_religion'] = df_raw.apply(lambda row: row['dm_religion'] if row['dm_religion'] != 'Otro' else row['dm_rel_otro'].strip().title(), axis=1)\n",
    "\n",
    "\n",
    "## Recode the different entries into believers-atheist/agnostics\n",
    "religion = ['Católico', 'Islámico', 'Evangélico', 'Protestante', 'Budista', 'Cristiano', 'Protestante', \n",
    "             'Musulmán', 'Grigoriano Apostolico', 'Soy Cristiana', 'Católica No Practicante', 'Testigo De Jehová', \n",
    "             'Católico Pero No Acudo A Misa', 'Soy Cristiano, Creo En Dios', 'Católico No Practicante',\n",
    "             'Es Largo De Explicar , Es Una Iglesia Moderna', 'Creo En Las Ciencias Ocultas Y La Espiritualidad',\n",
    "             'Creo En Dios A Través De Jesucristo',\n",
    "             'Sincretismo Pragmático', 'Creo En Dios A Través De Jesucristo.', 'Ortodoxo']\n",
    "\n",
    "no_religion = ['Agnóstico', 'Soy Agnóstico', 'Ateo', 'Ningún Punto Religioso..Creo En La Vida.', 'Ni Creo Ni Dejo De Creer' ]\n",
    "\n",
    "df_raw['Religion'] = df_raw['dm_pref_religion'].replace(religion, 'Religious')\n",
    "df_raw['Religion'] = df_raw['Religion'].replace(no_religion, 'No religious')\n",
    "# Code as None if not within these two Religion\n",
    "df_raw['Religion'] = np.where(df_raw['Religion'].isin(['Religious','No religious']), df_raw['Religion'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['dm_pref_religion'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-stable",
   "metadata": {
    "tags": []
   },
   "source": [
    "After recoding, we obtained the following repartition between `Believers` and `Atheists/Agnostics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['Religion'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-albany",
   "metadata": {},
   "source": [
    "### Recode politics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-evanescence",
   "metadata": {},
   "source": [
    "This variable has been recoded into `Izquierda`, `Centro`, `Derecha`, from the original question. \n",
    "The recoding is not necessarily needed. Again we can decide to change that later. \n",
    "\n",
    "* `Izquierda`: 'Izquierda', 'Centro izquierda'\n",
    "\n",
    "* `Centro`: 'Centro'\n",
    "\n",
    "* `Derecha`: 'Derecha', 'Centro derecha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-penalty",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw['dm_politica'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate politics preferences into bigger groups (izquierda, derecha, centro)\n",
    "def aggregate_politics(val):\n",
    "    if val in ['Izquierda', 'Centro izquierda']:\n",
    "        return 'Left'\n",
    "    elif val in ['Derecha', 'Centro derecha']:\n",
    "        return 'Right'\n",
    "    elif val == 'Centro':\n",
    "        return \"Centre\"\n",
    "    else:\n",
    "        return None\n",
    "df_raw['Political'] = df_raw['dm_politica'].apply(aggregate_politics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-selection",
   "metadata": {
    "tags": []
   },
   "source": [
    "After recoding, we obtained the following repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['Political'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-folder",
   "metadata": {},
   "source": [
    "### Recode age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-straight",
   "metadata": {},
   "source": [
    "Age as been recoded to group in 3 categories:\n",
    "\n",
    "* `<=18-34`: '< 18 años', '18-24 años', '25-34 años'\n",
    "\n",
    "* `35-54`: '35-44 años', '45-54 años'\n",
    "\n",
    "* `>55`: '55-65 años', '> 65 años'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95df22-0251-4420-8f8a-ba32e40615e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['dm_edad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate ages into bigger groups (18-34, 35-54, >54)\n",
    "def aggregate_age(val):\n",
    "    if val in ['< 18 años', '18-24 años', '25-34 años']:\n",
    "        return '<=18-34'\n",
    "    elif val in ['35-44 años', '45-54 años']:\n",
    "        return '35-54'\n",
    "    elif val in ['55-65 años', '> 65 años']:\n",
    "        return '>55'\n",
    "    elif val is None:\n",
    "        return None\n",
    "df_raw['Age'] = df_raw.dm_edad.apply(aggregate_age)\n",
    "\n",
    "df_raw['Age'] = pd.Categorical(df_raw['Age'], categories=['<=18-34', '35-54', '>55'],ordered=True)\n",
    "\n",
    "\n",
    "df_raw['Age'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-boulder",
   "metadata": {},
   "source": [
    "### Recode education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-breeding",
   "metadata": {},
   "source": [
    "Education has been recoded in two forms. One  is to group the subject into `Uni` vs `No Uni`. While the distinction makes sense regarding the data, it has been considered too *university-self-centerred*. Therefore we fall back on a three categories distinctions\n",
    "\n",
    "* `up_to_secondaria`: 'Primaria', 'Secundaria', 'No tiene estudios en educación formal'\n",
    "\n",
    "* `up_to_bac_3`: 'Formación Profesional', 'Bachillerato', 'Cou'\n",
    "\n",
    "* `up_to_uni`: 'Master','Posgrado', 'Doctorado',  'Grado/Licenciatura'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode education\n",
    "df_raw['dm_education'] = df_raw.apply(lambda row: row['dm_educacion'] if row['dm_educacion'] != 'Otro' else row['dm_edu_otro'].strip().title(), axis=1)\n",
    "\n",
    "\n",
    "#education_recoded_values = ['Primaria', 'Secundaria', 'Formación Profesional', 'Grado/Licenciatura', 'Master', 'Posgrado', 'Doctorado' ]\n",
    "\n",
    "# Code as None if not within the list of education level\n",
    "#df_raw['recode_education'] = np.where(df_raw['dm_education'].isin(education_recoded_values), df_raw['dm_education'], None)\n",
    "\n",
    "\n",
    "# Recode education into broader categories\n",
    "uni = ['Posgrado', 'Doctorado', 'Master', 'Grado/Licenciatura' ]\n",
    "non_uni = ['No tiene estudios en educación formal', 'Formación Profesional', 'Primaria', 'Secundaria', 'Cou', 'Bachillerato']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_raw['Education'] = df_raw['dm_education'].replace(uni, 'University studies')\n",
    "df_raw['Education'] = df_raw['Education'].replace(non_uni, 'No university studies')\n",
    "\n",
    "\n",
    "\n",
    "# Alternative recoding\n",
    "up_to_secondaria = ['Primaria', 'Secundaria', 'No tiene estudios en educación formal']\n",
    "up_to_bac_3 = ['Formación Profesional', 'Bachillerato', 'Cou']\n",
    "up_to_university_level = ['Master','Posgrado', 'Doctorado',  'Grado/Licenciatura']\n",
    "\n",
    "\n",
    "# Code as None if not within these categories\n",
    "df_raw['Education'] = np.where(df_raw['Education'].isin(['University studies', 'No university studies']), df_raw['Education'], None)\n",
    "df_raw['Education'].value_counts()\n",
    "\n",
    "df_raw['Education2'] = df_raw['dm_education'].replace(up_to_secondaria, 'Secondary')\n",
    "df_raw['Education2'] = df_raw['Education2'].replace(up_to_bac_3, 'College')\n",
    "df_raw['Education2'] = df_raw['Education2'].replace(up_to_university_level, 'University')\n",
    "df_raw['Education2'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36215b12-a53f-4058-b685-3e1d4c34e994",
   "metadata": {},
   "source": [
    "### Recode Technological Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e32ede-3571-4817-bc9a-a50b2016a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.replace({\"Avanzada\": \"Advanced\", 'Media': \"Intermediate\", \"Básica\": \"Basic\"})\n",
    "df_raw['Technological'] = df_raw['dm_tecnologia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-variety",
   "metadata": {},
   "source": [
    "### Recode actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faf and taf are questions asked if the subject answer 'True' to the news in case of True and Fake news. \n",
    "# The faf will then for people who were wrong and the taf for people who were rignt\n",
    "var_actions_f = ['faf_compartira_familia_amigos', 'faf_publicara_redes', 'faf_consultara_fuentes', 'faf_aplicara_aprendido', 'faf_no_accion']\n",
    "var_actions_t = ['taf_compartira_familia_amigos', 'taf_publicara_redes', 'taf_consultara_fuentes', 'taf_aplicara_aprendido', 'taf_no_accion']\n",
    "var_actions = var_actions_f + var_actions_t\n",
    "df_raw.loc[:, var_actions] = df_raw.loc[:, var_actions].replace({'checked': True, 'unchecked': False})\n",
    "df_raw.loc[:, var_actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-philadelphia",
   "metadata": {},
   "source": [
    "There is an issue with some subjects answering the no_action while answering other types of action at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an issue with some subjects answering the no_action while answering other types of action at the same time\n",
    "df_raw['total_checked_f'] = df_raw.loc[:, var_actions_f].isin([True]).sum(axis=1)\n",
    "df_raw['total_checked_t'] = df_raw.loc[:, var_actions_t].isin([True]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the subjects when conflicting answer in case of analysing actions. \n",
    "df_raw[(df_raw['total_checked_f'] > 1) & (df_raw['faf_no_accion'] == True)][['total_checked_f', *var_actions_f]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the subjects when conflicting answer in case of analysing actions. \n",
    "\n",
    "for x in var_actions_f:\n",
    "    df_raw.loc[(df_raw['total_checked_f'] > 1) & (df_raw['faf_no_accion'] == True), x] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[(df_raw['total_checked_t'] > 1) & (df_raw['taf_no_accion'] == True)][['total_checked_t', *var_actions_t]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the subjects when conflicting answer in case of analysing actions. \n",
    "for x in var_actions_t:\n",
    "    df_raw.loc[(df_raw['total_checked_t'] > 1) & (df_raw['taf_no_accion'] == True), x] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-repair",
   "metadata": {},
   "source": [
    "### Recode justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename two columns for tys that are different than the column from fys and create issue later in analysis\n",
    "df_raw = df_raw.rename(columns={'tys_medio_comunicacion_conocido': 'tys_medio_conocido', 'tys_medio_comunicacion_fiable': 'tys_medio_fiable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_justification_fake = ['tno_aclaracion_desmintiendo',\n",
    "'tno_medio_desconocido',\n",
    "'tno_medio_poco_fiable',\n",
    "'tno_fuentes_desconocidas',\n",
    "'tno_fuentes_no_confiables',\n",
    "'tno_sin_fuentes',\n",
    "'tno_abordaje_no_serio',\n",
    "'tno_no_coherente',\n",
    "'tno_titulo_sensacionalista',\n",
    "'tno_imagen_sensacionalista',\n",
    "'tno_no_concuerda_creencias',\n",
    "'tno_no_alineado_ideologia',\n",
    "'tno_otro','fno_aclaracion_desmintiendo',\n",
    "'fno_medio_desconocido',\n",
    "'fno_medio_poco_fiable',\n",
    "'fno_fuentes_desconocidas',\n",
    "'fno_fuentes_no_confiables',\n",
    "'fno_sin_fuentes',\n",
    "'fno_abordaje_no_serio',\n",
    "'fno_no_coherente',\n",
    "'fno_titulo_sensacionalista',\n",
    "'fno_imagen_sensacionalista',\n",
    "'fno_no_concuerda_creencias',\n",
    "'fno_no_alineado_ideologia',\n",
    "'fno_otro']\n",
    "var_justification_true = ['fys_recuerda_leida',\n",
    "'fys_medio_conocido',\n",
    "'fys_medio_fiable',\n",
    "'fys_fuentes_conocidas',\n",
    "'fys_fuentes_confiables',\n",
    "'fys_abordaje_serio',\n",
    "'fys_coherente',\n",
    "'fys_concuerda_creencias',\n",
    "'fys_alineado_ideologia',\n",
    "'fys_otro',\n",
    "'tys_recuerda_leida',\n",
    "'tys_medio_conocido',\n",
    "'tys_medio_fiable',\n",
    "'tys_fuentes_conocidas',\n",
    "'tys_fuentes_confiables',\n",
    "'tys_abordaje_serio',\n",
    "'tys_coherente',\n",
    "'tys_concuerda_creencias',\n",
    "'tys_alineado_ideologia',\n",
    "'tys_otro']\n",
    "\n",
    "var_justifications = var_justification_true + var_justification_fake\n",
    "df_raw.loc[:, var_justifications] = df_raw.loc[:, var_justifications].replace({'checked': True, 'unchecked': False, 'undisplayed': np.NaN})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-continent",
   "metadata": {},
   "source": [
    "### Recode NS/NC into Np.NaN\n",
    "It seems the NS/NC is equal to `no answer`. We recoded globally accordingly. The consequence in all further analysis is that people who did not answer the question will be removed from the analysis. They are removed for the analysis they did not provide an answer, not removed from the dataset (therefore we will have different N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NS/NC as np.nan to exclude subject in analysis\n",
    "df_raw = df_raw.replace('NS/NC', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-employee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-hospital",
   "metadata": {},
   "source": [
    "### On survey completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-algorithm",
   "metadata": {
    "tags": []
   },
   "source": [
    "To decide which data are considered as completed, the time to reach `time_news2` is use and it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-receipt",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #FIXME The date_finish is never None. Therefore that datapoint is not relevant. \n",
    "fin = ~df_raw['date_finish'].isnull() # finished experiments\n",
    "ini = df_raw['time_index']>0          # initiated or read news 1\n",
    "read_only_1 = ini & (df_raw['time_news1']==0) # read only 1 news\n",
    "read_both = df_raw['time_news1']>0    # read both news\n",
    "got_to_ans = df_raw['time_news2']>0    # read both news and got to ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-natural",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtering\n",
    "df_all = df_raw\n",
    "df_init = df_raw[ini]\n",
    "df = df_raw[got_to_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-shift",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = ['Han entrado','Han iniciado (y leído la primera noticia)','Han leído la segunda noticia', 'Han llegado al formulario', 'Han finalizado']\n",
    "x = [df_all.shape[0], df_all[ini].shape[0], df_all[read_both].shape[0], df_all[got_to_ans].shape[0], df_all[fin].shape[0]]\n",
    "plt.barh(y, x)\n",
    "for index, value in enumerate(y):\n",
    "    plt.text(x[index], index, str(x[index]))\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Frecuencia de usuarios por estado')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-juice",
   "metadata": {},
   "source": [
    "The column `Han finalizado` is inconsistent with the other counts. After inspection it is because there is no `None` value for that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[got_to_ans]\n",
    "\n",
    "print('Size of the filtered sample: {}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-queens",
   "metadata": {},
   "source": [
    "### On time completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put the outliers in the first and last bin\n",
    "# Source: https://stackoverflow.com/a/51050772\n",
    "def outlier_aware_hist(data, lower=None, upper=None):\n",
    "    if not lower or lower < data.min():\n",
    "        lower = data.min()\n",
    "        lower_outliers = False\n",
    "    else:\n",
    "        lower_outliers = True\n",
    "\n",
    "    if not upper or upper > data.max():\n",
    "        upper = data.max()\n",
    "        upper_outliers = False\n",
    "    else:\n",
    "        upper_outliers = True\n",
    "\n",
    "    n, bins, patches = plt.hist(data, range=(lower, upper), bins='auto')\n",
    "\n",
    "    if lower_outliers:\n",
    "        n_lower_outliers = (data < lower).sum()\n",
    "        patches[0].set_height(patches[0].get_height() + n_lower_outliers)\n",
    "        patches[0].set_facecolor('c')\n",
    "        patches[0].set_label('Lower outliers: ({:.2f}, {:.2f})'.format(data.min(), lower))\n",
    "\n",
    "    if upper_outliers:\n",
    "        n_upper_outliers = (data > upper).sum()\n",
    "        patches[-1].set_height(patches[-1].get_height() + n_upper_outliers)\n",
    "        patches[-1].set_facecolor('m')\n",
    "        patches[-1].set_label('Upper outliers: ({:.2f}, {:.2f})'.format(upper, data.max()))\n",
    "\n",
    "    if lower_outliers or upper_outliers:\n",
    "        plt.legend()\n",
    "\n",
    "def mad(data):\n",
    "    median = np.median(data)\n",
    "    diff = np.abs(data - median)\n",
    "    mad = np.median(diff)\n",
    "    return mad\n",
    "\n",
    "def calculate_bounds(data, z_thresh=3.5):\n",
    "    MAD = mad(data)\n",
    "    median = np.median(data)\n",
    "    const = z_thresh * MAD / 0.6745\n",
    "    return (median - const, median + const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "outlier_aware_hist(df['time_completion_min'], lower = 2, upper=25)\n",
    "#plt.figtext(0.65,0.6, df['time_completion_min'].describe().to_string())\n",
    "\n",
    "plt.title(\"Time of completion in minutes\")\n",
    "#ax.legend().set_visible(False)\n",
    "fig.savefig('./plots/time_completion.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['time_completion_min']<=20]['time_completion_min'].plot.hist()\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(df[df['time_completion_min']<=20]['time_completion_min'], )\n",
    "plt.figtext(0.65,0.6, df[df['time_completion_min']<=20]['time_completion_min'].describe().to_string())\n",
    "plt.title(\"Time of completion in minutes\")\n",
    "fig.savefig('./plots/time_completion.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-knight",
   "metadata": {},
   "source": [
    "After looking at the data for the time completion, we decided to remove any subject that took less than 2 minutes to answer the survey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the subjects that answer in less or equal to 1 minutes to be sure we get subject that get focused. \n",
    "df[df['time_completion_min'] < 2]['time_completion_min'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['time_completion_min'] > 45]['time_completion_min'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-borough",
   "metadata": {},
   "source": [
    "## Final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['time_completion_min'] > 2) & (df['time_completion_min'] <25)]\n",
    "print('Size of the final filtered sample: {}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-painting",
   "metadata": {},
   "source": [
    "# Dataset from news perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-bhutan",
   "metadata": {},
   "source": [
    "## Melt the sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the main df into df by news \n",
    "df_news = df.melt(id_vars=[*[i for i in df.columns if i not in ['fake_news', 'true_news']]])\n",
    "\n",
    "df_news = df_news.rename(columns={'variable': 'type_news', 'value': 'news_title'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the answer for each answer\n",
    "df_news['answer'] = np.where(df_news['type_news'] == 'fake_news', df_news['Fake news'],\n",
    "         (np.where(df_news['type_news'] == 'true_news', df_news['True news'], np.NaN)))\n",
    "\n",
    "## Drop Fake news and True News\n",
    "\n",
    "df_news = df_news.drop(columns=['first_true', 'reread_fake', 'Fake news', 'True news'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-sociology",
   "metadata": {},
   "source": [
    "## Add the justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the subject thinks it is a Fake News (regardless of the news)\n",
    "var_justification_true_wrong = ['tno_aclaracion_desmintiendo',\n",
    "'tno_medio_desconocido',\n",
    "'tno_medio_poco_fiable',\n",
    "'tno_fuentes_desconocidas',\n",
    "'tno_fuentes_no_confiables',\n",
    "'tno_sin_fuentes',\n",
    "'tno_abordaje_no_serio',\n",
    "'tno_no_coherente',\n",
    "'tno_titulo_sensacionalista',\n",
    "'tno_imagen_sensacionalista',\n",
    "'tno_no_concuerda_creencias',\n",
    "'tno_no_alineado_ideologia',\n",
    "'tno_otro']\n",
    "\n",
    "var_justification_fake_right = ['fno_aclaracion_desmintiendo',\n",
    "'fno_medio_desconocido',\n",
    "'fno_medio_poco_fiable',\n",
    "'fno_fuentes_desconocidas',\n",
    "'fno_fuentes_no_confiables',\n",
    "'fno_sin_fuentes',\n",
    "'fno_abordaje_no_serio',\n",
    "'fno_no_coherente',\n",
    "'fno_titulo_sensacionalista',\n",
    "'fno_imagen_sensacionalista',\n",
    "'fno_no_concuerda_creencias',\n",
    "'fno_no_alineado_ideologia',\n",
    "'fno_otro']\n",
    "\n",
    "## Creating it for the fake news\n",
    "df_fake = df_news[df_news['type_news'] == 'fake_news'][[ 'news_title', *var_justification_fake_right]]\n",
    "df_fake = df_fake.replace({True: 1, False: 0})\n",
    "df_fake = df_fake.groupby('news_title').agg('mean')\n",
    "df_fake.columns = ['justification_avg_' + '_'.join(i.split('_')[1:]) for i in df_fake.columns]\n",
    "\n",
    "## Creating for the true news\n",
    "df_true = df_news[df_news['type_news'] == 'true_news'][['news_title', *var_justification_true_wrong]]\n",
    "df_true = df_true.replace({True: 1, False: 0})\n",
    "df_true = df_true.groupby('news_title').agg('mean')\n",
    "df_true.columns = ['justification_avg_' + '_'.join(i.split('_')[1:]) for i in df_true.columns]\n",
    "\n",
    "## Append the two dataframe\n",
    "df_avg_justification_fake = df_fake\n",
    "df_avg_justification_fake = df_avg_justification_fake.append(df_true)\n",
    "df_avg_justification_fake = df_avg_justification_fake.reset_index().rename(columns={'index': 'news_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cases the subject is right for true news and wrong for fake news\n",
    "var_justification_fake_wrong = ['fys_recuerda_leida',\n",
    "'fys_medio_conocido',\n",
    "'fys_medio_fiable',\n",
    "'fys_fuentes_conocidas',\n",
    "'fys_fuentes_confiables',\n",
    "'fys_abordaje_serio',\n",
    "'fys_coherente',\n",
    "'fys_concuerda_creencias',\n",
    "'fys_alineado_ideologia',\n",
    "'fys_otro']\n",
    "\n",
    "var_justification_true_right = ['tys_recuerda_leida',\n",
    "'tys_medio_conocido',\n",
    "'tys_medio_fiable',\n",
    "'tys_fuentes_conocidas',\n",
    "'tys_fuentes_confiables',\n",
    "'tys_abordaje_serio',\n",
    "'tys_coherente',\n",
    "'tys_concuerda_creencias',\n",
    "'tys_alineado_ideologia',\n",
    "'tys_otro']\n",
    "\n",
    "## Creating it for the fake news\n",
    "df_fake = df_news[df_news['type_news'] == 'fake_news'][[ 'news_title', *var_justification_fake_wrong]]\n",
    "df_fake = df_fake.replace({True: 1, False: 0})\n",
    "df_fake = df_fake.groupby('news_title').agg('mean')\n",
    "df_fake.columns = ['justification_avg_' + '_'.join(i.split('_')[1:]) for i in df_fake.columns]\n",
    "\n",
    "## Creating for the true news\n",
    "df_true = df_news[df_news['type_news'] == 'true_news'][['news_title', *var_justification_true_right]]\n",
    "df_true = df_true.replace({True: 1, False: 0})\n",
    "df_true = df_true.groupby('news_title').agg('mean')\n",
    "df_true.columns = ['justification_avg_' + '_'.join(i.split('_')[1:]) for i in df_true.columns]\n",
    "\n",
    "## Append the two dataframes\n",
    "df_avg_justification_true = df_fake\n",
    "df_avg_justification_true = df_avg_justification_true.append(df_true)\n",
    "df_avg_justification_true = df_avg_justification_true.reset_index().rename(columns={'index': 'news_title'})\n",
    "\n",
    "# Merging all the possibilities\n",
    "df_avg_justification = df_avg_justification_fake.merge(df_avg_justification_true, on='news_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-arrangement",
   "metadata": {},
   "source": [
    "## Add the different actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_actions_f = ['faf_compartira_familia_amigos', 'faf_publicara_redes', 'faf_consultara_fuentes', 'faf_aplicara_aprendido', 'faf_no_accion']\n",
    "var_actions_t = ['taf_compartira_familia_amigos', 'taf_publicara_redes', 'taf_consultara_fuentes', 'taf_aplicara_aprendido', 'taf_no_accion']\n",
    "\n",
    "\n",
    "## Creating it for the fake news\n",
    "df_fake = df_news[df_news['type_news'] == 'fake_news'][[ 'news_title', *var_actions_f]]\n",
    "df_fake = df_fake.replace({True: 1, False: 0})\n",
    "df_fake = df_fake.groupby('news_title').agg('mean')\n",
    "df_fake.columns = ['action_avg_' + '_'.join(i.split('_')[1:]) for i in df_fake.columns]\n",
    "\n",
    "## Creating for the true news\n",
    "df_true = df_news[df_news['type_news'] == 'true_news'][['news_title', *var_actions_t]]\n",
    "df_true = df_true.replace({True: 1, False: 0})\n",
    "df_true = df_true.groupby('news_title').agg('mean')\n",
    "df_true.columns = ['action_avg_' + '_'.join(i.split('_')[1:]) for i in df_true.columns]\n",
    "\n",
    "## Append the two dataframe\n",
    "df_avg_action = df_fake\n",
    "df_avg_action = df_avg_action.append(df_true)\n",
    "df_avg_action = df_avg_action.reset_index().rename(columns={'index': 'news_title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-legend",
   "metadata": {},
   "source": [
    "## Add all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing all the uneeded columns \n",
    "df_news_details = df_news[['news_title', 'type_news']]\n",
    "\n",
    "# Drop duplicated rows as from the df_news\n",
    "df_news_details = df_news_details.drop_duplicates()\n",
    "\n",
    "\n",
    "#### Merge with the topics\n",
    "df_news_details = df_news_details.merge(df_topic, on='news_title')\n",
    "\n",
    "\n",
    "#### Merge the justification\n",
    "df_news_details = df_news_details.merge(df_avg_justification, on='news_title')\n",
    "\n",
    "\n",
    "#### Merge the actions\n",
    "df_news_details = df_news_details.merge(df_avg_action, on='news_title')\n",
    "\n",
    "\n",
    "### Merge the resulting df with the info by news\n",
    "\n",
    "df_news = df_news.merge(df_news_details, on='news_title', how='left')\n",
    "\n",
    "\n",
    "#### Drop the type_news_y and rename type_news_x into type_news (they are identical after checking)\n",
    "df_news = df_news.drop(columns=['type_news_y'])\n",
    "df_news = df_news.rename(columns={'type_news_x': 'type_news'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the columns about actions to get all under the same title and remove the uneeded columns\n",
    "for col in ['compartira_familia_amigos', 'publicara_redes', 'consultara_fuentes', 'aplicara_aprendido', 'no_accion']:\n",
    "    wrong_prefix = 'faf_'\n",
    "    right_prefix = 'taf_'\n",
    "    right_condition = ((df_news['type_news'] == 'true_news') & (df_news['answer'] == 'Right'))\n",
    "    wrong_condition = ((df_news['type_news'] == 'fake_news') & (df_news['answer'] == 'Wrong'))\n",
    "    df_news[col] = np.where(right_condition, \n",
    "                            df_news['{}{}'.format(right_prefix, col)],\n",
    "                            (np.where(wrong_condition, \n",
    "                                      df_news['{}{}'.format(wrong_prefix, col)], None)))\n",
    "\n",
    "                                      \n",
    "## Remove all the unneeded columns\n",
    "df_news = df_news.drop(columns=[ 'taf_compartira_familia_amigos', \n",
    "                                'taf_publicara_redes', \n",
    "                                'taf_consultara_fuentes', \n",
    "                                'taf_aplicara_aprendido', \n",
    "                                'taf_no_accion', 'faf_compartira_familia_amigos', 'faf_publicara_redes', 'faf_consultara_fuentes', 'faf_aplicara_aprendido', 'faf_no_accion'])\n",
    "\n",
    "## Rename the columns in English\n",
    "to_rename = {'compartira_familia_amigos': 'share_friends_and_family',\n",
    "             'publicara_redes': 'share_online',\n",
    "             'consultara_fuentes': 'verify_source',\n",
    "             'aplicara_aprendido': \"apply_learning\",\n",
    "             'no_accion': 'no_action'}\n",
    "df_news = df_news.rename(columns=to_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the columns about justification to get all under the same title and remove the uneeded columns\n",
    "\n",
    "var_justification_fake = ['tno_aclaracion_desmintiendo',\n",
    "                          'tno_medio_desconocido',\n",
    "                          'tno_medio_poco_fiable',\n",
    "                          'tno_fuentes_desconocidas',\n",
    "                          'tno_fuentes_no_confiables',\n",
    "                          'tno_sin_fuentes',\n",
    "                          'tno_abordaje_no_serio',\n",
    "                          'tno_no_coherente',\n",
    "                          'tno_titulo_sensacionalista',\n",
    "                          'tno_imagen_sensacionalista',\n",
    "                          'tno_no_concuerda_creencias',\n",
    "                          'tno_no_alineado_ideologia',\n",
    "                          'tno_otro',\n",
    "                          'fno_aclaracion_desmintiendo',\n",
    "                          'fno_medio_desconocido',\n",
    "                          'fno_medio_poco_fiable',\n",
    "                          'fno_fuentes_desconocidas',\n",
    "                          'fno_fuentes_no_confiables',\n",
    "                          'fno_sin_fuentes',\n",
    "                          'fno_abordaje_no_serio',\n",
    "                          'fno_no_coherente',\n",
    "                          'fno_titulo_sensacionalista',\n",
    "                          'fno_imagen_sensacionalista',\n",
    "                          'fno_no_concuerda_creencias',\n",
    "                          'fno_no_alineado_ideologia',\n",
    "                          'fno_otro']\n",
    "\n",
    "var_justification_true = ['fys_recuerda_leida',\n",
    "                          'fys_medio_conocido',\n",
    "                          'fys_medio_fiable',\n",
    "                          'fys_fuentes_conocidas',\n",
    "                          'fys_fuentes_confiables',\n",
    "                          'fys_abordaje_serio',\n",
    "                          'fys_coherente',\n",
    "                          'fys_concuerda_creencias',\n",
    "                          'fys_alineado_ideologia',\n",
    "                          'fys_otro',\n",
    "                          'tys_recuerda_leida',\n",
    "                          'tys_medio_conocido',\n",
    "                          'tys_medio_fiable',\n",
    "                          'tys_fuentes_conocidas',\n",
    "                          'tys_fuentes_confiables',\n",
    "                          'tys_abordaje_serio',\n",
    "                          'tys_coherente',\n",
    "                          'tys_concuerda_creencias',\n",
    "                          'tys_alineado_ideologia',\n",
    "                          'tys_otro']\n",
    "\n",
    "var_just_fake = ['aclaracion_desmintiendo',\n",
    "                 'medio_desconocido',\n",
    "                 'medio_poco_fiable',\n",
    "                 'fuentes_desconocidas',\n",
    "                 'fuentes_no_confiables',\n",
    "                 'sin_fuentes',\n",
    "                 'abordaje_no_serio',\n",
    "                 'no_coherente',\n",
    "                 'titulo_sensacionalista',\n",
    "                 'imagen_sensacionalista',\n",
    "                 'no_concuerda_creencias',\n",
    "                 'no_alineado_ideologia',\n",
    "                 'otro']\n",
    "\n",
    "var_just_true = ['recuerda_leida',\n",
    "                 'medio_conocido',\n",
    "                 'medio_fiable',\n",
    "                 'fuentes_conocidas',\n",
    "                 'fuentes_confiables',\n",
    "                 'abordaje_serio',\n",
    "                 'coherente',\n",
    "                 'concuerda_creencias',\n",
    "                 'alineado_ideologia',\n",
    "                 'otro']\n",
    "\n",
    "# TNO: True news and Wrong answer\n",
    "# FNO: Fake news and Right answer\n",
    "for col in var_just_fake:\n",
    "    wrong_prefix = 'fno_'\n",
    "    right_prefix = 'tno_'\n",
    "    right_condition = ((df_news['type_news'] == 'fake_news') & (df_news['answer'] == 'Right'))\n",
    "    wrong_condition = ((df_news['type_news'] == 'true_news') & (df_news['answer'] == 'Wrong'))\n",
    "    df_news[col] = np.where(right_condition, \n",
    "                            df_news['{}{}'.format(right_prefix, col)],\n",
    "                            (np.where(wrong_condition, \n",
    "                                      df_news['{}{}'.format(wrong_prefix, col)], None)))\n",
    "\n",
    "                                      \n",
    "## Remove all the unneeded columns\n",
    "df_news = df_news.drop(columns=var_justification_fake)\n",
    "\n",
    "# FYS: Fake news and Wrong answer\n",
    "# TYS: True news and Right answer \n",
    "for col in var_just_true:\n",
    "    wrong_prefix = 'fys_'\n",
    "    right_prefix = 'tys_'\n",
    "    right_condition = ((df_news['type_news'] == 'true_news') & (df_news['answer'] == 'Right'))\n",
    "    wrong_condition = ((df_news['type_news'] == 'fake_news') & (df_news['answer'] == 'Wrong'))\n",
    "    df_news[col] = np.where(right_condition, \n",
    "                            df_news['{}{}'.format(right_prefix, col)],\n",
    "                            (np.where(wrong_condition, \n",
    "                                      df_news['{}{}'.format(wrong_prefix, col)], None)))\n",
    "\n",
    "## Remove all the unneeded columns\n",
    "df_news = df_news.drop(columns=var_justification_true)\n",
    "\n",
    "## Rename the different justifications in English\n",
    "var_trans_fake = {'aclaracion_desmintiendo': \"Previously_read_debunked\",\n",
    "                 'medio_desconocido': \"Source_unknown\",\n",
    "                 'medio_poco_fiable': \"Media_unreliable\",\n",
    "                 'fuentes_desconocidas': \"Cited_sources_unknown\",\n",
    "                 'fuentes_no_confiables': \"Cited_sources_unreliable\",\n",
    "                 'sin_fuentes': \"Without_sources\",\n",
    "                 'abordaje_no_serio': \"Unprofessional_style\",\n",
    "                 'no_coherente': \"No_coherent\",\n",
    "                 'titulo_sensacionalista': \"Headline_sensationalist\",\n",
    "                 'imagen_sensacionalista': \"Image_sensationalist\",\n",
    "                 'no_concuerda_creencias':\"Different_belief\",\n",
    "                 'no_alineado_ideologia':\"Different_ideology\",\n",
    "                 'otro': \"Other\"}\n",
    "\n",
    "var_trans_true = {'recuerda_leida': \"Previously_read_the_information\",\n",
    "                 'medio_conocido': \"Known_media\",\n",
    "                 'medio_fiable': \"Reliable_media\",\n",
    "                 'fuentes_conocidas':\"Source_known\",\n",
    "                 'fuentes_confiables':\"Source_Reliable\",\n",
    "                 'abordaje_serio': \"Professional_style\",\n",
    "                 'coherente': \"Coherent\",\n",
    "                 'concuerda_creencias':\"Same_belief\",\n",
    "                 'alineado_ideologia':\"Same_ideology\",\n",
    "                 'otro': \"Other\"}\n",
    "\n",
    "df_news = df_news.rename(columns=var_trans_fake)\n",
    "df_news = df_news.rename(columns=var_trans_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-input",
   "metadata": {},
   "source": [
    "# Records the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset into a clean csv file for further analysis\n",
    "df.to_csv('{}{}'.format(data_folder, dataset_clean_filename),  index = False)\n",
    "\n",
    "# Save the df_news_information\n",
    "#df_news_details.to_csv('{}{}'.format(data_folder, dataset_news_filename), index=False)\n",
    "\n",
    "# Save the df_news_full\n",
    "df_news.to_csv('{}{}'.format(data_folder, dataset_news_full_filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-preference",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "## Packages\n",
    "\n",
    " * mord: [https://github.com/fabianp/mord](https://github.com/fabianp/mord)\n",
    "     >Pedregosa, Fabian, Francis Bach, and Alexandre Gramfort. \"On the consistency of ordinal regression methods.\" The Journal of Machine Learning Research 18.1 (2017) JMLR.\n",
    " * pingouin: [https://pingouin-stats.org/](https://pingouin-stats.org/)\n",
    "     > Vallat, R. (2018). Pingouin: statistics in Python. Journal of Open Source Software, 3(31), 1026, https://doi.org/10.21105/joss.01026\n",
    " * statsmodels: [https://www.statsmodels.org](https://www.statsmodels.org)\n",
    "    > @inproceedings{seabold2010statsmodels,title={statsmodels: Econometric and statistical modeling with python}, author={Seabold, Skipper and Perktold, Josef}, booktitle={9th Python in Science Conference},year={2010},}\n",
    " * scipy: [https://www.scipy.org](https://www.scipy.org)\n",
    "     > @ARTICLE{2020SciPy-NMeth, author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\\.I}lhan and Feng, Yu and Moore, Eric W. and {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}}, journal = {Nature Methods}, year    = {2020}, volume  = {17}, pages   = {261--272}, adsurl  = {https://rdcu.be/b08Wh}, doi     = {10.1038/s41592-019-0686-2},}\n",
    " * scikit-learn: [https://scikit-learn.org](https://scikit-learn.org)\n",
    "     > Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay. Scikit-learn: Machine Learning in Python, Journal of Machine Learning Research, 12, 2825-2830 (2011) (publisher link)\n",
    "     \n",
    " * pymer4: [https://eshinjolly.com/pymer4/]\n",
    "     > Jolly, (2018). Pymer4: Connecting R and Python for Linear Mixed Modeling. Journal of Open Source Software, 3(31), 862, https://doi.org/10.21105/joss.00862\n",
    "\n",
    "## Articles\n",
    "\n",
    "*   Bürkner, P.-C., & Vuorre, M. (2019). Ordinal Regression Models in Psychology: A Tutorial. Advances in Methods and Practices in Psychological Science, 77–101. https://doi.org/10.1177/2515245918823199"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
